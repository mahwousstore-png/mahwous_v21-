"""
engines/engine.py â€” Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© v21
Ù…Ù†Ø·Ù‚: Fuzzy Ø³Ø±ÙŠØ¹ â†’ Gemini Ù„Ù„ØºØ§Ù…Ø¶ (62-96%) â†’ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ÙˆØ§Ø¶Ø­ (97%+)
"""
import re, io, json, hashlib, sqlite3, time
from datetime import datetime
import pandas as pd
from rapidfuzz import fuzz, process as rf_process

try:
    from config import (REJECT_KEYWORDS, ALL_BRANDS, BRANDS_EN, BRANDS_AR,
                        KNOWN_BRANDS, WORD_REPLACEMENTS,
                        MATCH_THRESHOLD, AUTO_THRESHOLD, HIGH_CONFIDENCE,
                        REVIEW_THRESHOLD, PRICE_TOLERANCE,
                        TESTER_KEYWORDS, SET_KEYWORDS, GEMINI_API_KEYS,
                        DB_PATH, AI_BATCH_SIZE)
except Exception:
    REJECT_KEYWORDS = ["sample","Ø¹ÙŠÙ†Ø©","Ø¹ÙŠÙ†Ù‡","decant","ØªÙ‚Ø³ÙŠÙ…","split","miniature"]
    ALL_BRANDS = KNOWN_BRANDS = BRANDS_EN = BRANDS_AR = []
    WORD_REPLACEMENTS = {}
    MATCH_THRESHOLD = 62; AUTO_THRESHOLD = 97; HIGH_CONFIDENCE = 92
    REVIEW_THRESHOLD = 75; PRICE_TOLERANCE = 10
    TESTER_KEYWORDS = ["tester","ØªØ³ØªØ±"]; SET_KEYWORDS = ["set","Ø·Ù‚Ù…","Ù…Ø¬Ù…ÙˆØ¹Ø©"]
    GEMINI_API_KEYS = []; DB_PATH = "mahwous.db"; AI_BATCH_SIZE = 12

import requests as _req

_GURL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

# â•â• Ù…Ø±Ø§Ø¯ÙØ§Øª Ø§Ù„Ø¹Ø·ÙˆØ± â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_SYN = {
    "eau de parfum":"edp","Ø§Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp","Ø£Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp",
    "Ø§Ùˆ Ø¯ÙŠ Ø¨Ø§Ø±ÙØ§Ù†":"edp","Ø¨Ø§Ø±ÙØ§Ù†":"edp","parfum":"edp","perfume":"edp",
    "eau de toilette":"edt","Ø§Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt","Ø£Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt",
    "ØªÙˆØ§Ù„ÙŠØª":"edt","toilette":"edt","toilet":"edt",
    "eau de cologne":"edc","ÙƒÙˆÙ„ÙˆÙ†":"edc","cologne":"edc",
    "extrait de parfum":"extrait","parfum extrait":"extrait",
    "Ø¯ÙŠÙˆØ±":"dior","Ø´Ø§Ù†ÙŠÙ„":"chanel","Ø£Ø±Ù…Ø§Ù†ÙŠ":"armani","ÙØ±Ø³Ø§ØªØ´ÙŠ":"versace",
    "ØºÙŠØ±Ù„Ø§Ù†":"guerlain","ØªÙˆÙ… ÙÙˆØ±Ø¯":"tom ford","Ù„Ø·Ø§ÙØ©":"lattafa","Ù„Ø·Ø§ÙÙ‡":"lattafa",
    "Ø£Ø¬Ù…Ù„":"ajmal","Ø±ØµØ§ØµÙŠ":"rasasi","Ø£Ù…ÙˆØ§Ø¬":"amouage","ÙƒØ±ÙŠØ¯":"creed",
    "Ù‚ÙˆØªØ´ÙŠ":"gucci","Ø¨Ø±Ø§Ø¯Ø§":"prada","Ù‡ÙŠØ±Ù…ÙŠØ³":"hermes","ÙØ§Ù„Ù†ØªÙŠÙ†Ùˆ":"valentino",
    "ÙƒØ§Ø±ØªÙŠÙŠÙ‡":"cartier","Ø¨ÙˆÙ„ØºØ§Ø±ÙŠ":"bvlgari","Ø¯ÙŠØ²Ù„":"diesel","Ø³ÙˆÙØ§Ø¬":"sauvage",
    "Ø¨Ù„Ùˆ":"bleu","ÙˆØ§Ù† Ù…ÙŠÙ„ÙŠÙˆÙ†":"1 million","Ø¥Ù†ÙÙŠÙƒØªÙˆØ³":"invictus",
    "Ø£ÙÙŠÙ†ØªÙˆØ³":"aventus","Ø¹ÙˆØ¯":"oud","Ù…Ø³Ùƒ":"musk",
    " Ù…Ù„":" ml","Ù…Ù„ÙŠ ":"ml ","Ù…Ù„ÙŠ":"ml","Ù…Ù„":"ml",
    "Ø£":"Ø§","Ø¥":"Ø§","Ø¢":"Ø§","Ø©":"Ù‡","Ù‰":"ÙŠ","Ø¤":"Ùˆ","Ø¦":"ÙŠ",
}

# â•â• SQLite Cache â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _init_db():
    try:
        cn = sqlite3.connect(DB_PATH, check_same_thread=False)
        cn.execute("CREATE TABLE IF NOT EXISTS ai_cache(h TEXT PRIMARY KEY, v TEXT)")
        cn.commit(); cn.close()
    except Exception:
        pass

def _cget(k):
    try:
        cn = sqlite3.connect(DB_PATH, check_same_thread=False)
        r = cn.execute("SELECT v FROM ai_cache WHERE h=?", (k,)).fetchone()
        cn.close()
        return json.loads(r[0]) if r else None
    except Exception:
        return None

def _cset(k, v):
    try:
        cn = sqlite3.connect(DB_PATH, check_same_thread=False)
        cn.execute("INSERT OR REPLACE INTO ai_cache VALUES(?,?)", (k, json.dumps(v, ensure_ascii=False)))
        cn.commit(); cn.close()
    except Exception:
        pass

_init_db()

# â•â• Ø¯ÙˆØ§Ù„ Ø£Ø³Ø§Ø³ÙŠØ© â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def read_file(f):
    """Ù‚Ø±Ø§Ø¡Ø© CSV/Excel Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    try:
        name = f.name.lower()
        if name.endswith('.csv'):
            for enc in ['utf-8', 'utf-8-sig', 'windows-1256', 'cp1256', 'latin-1']:
                try:
                    f.seek(0)
                    df = pd.read_csv(f, encoding=enc, on_bad_lines='skip')
                    if len(df) > 0: break
                except Exception:
                    continue
        elif name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(f)
        else:
            return None, "ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© â€” CSV Ø£Ùˆ Excel ÙÙ‚Ø·"
        df.columns = df.columns.str.strip()
        return df.dropna(how='all').reset_index(drop=True), None
    except Exception as e:
        return None, str(e)


def normalize(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ"""
    if not isinstance(text, str): return ""
    t = text.strip().lower()
    for k, v in WORD_REPLACEMENTS.items():
        t = t.replace(k.lower(), v)
    for k, v in _SYN.items():
        t = t.replace(k, v)
    t = re.sub(r'[^\w\s\u0600-\u06FF.]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()


def extract_size(text):
    if not isinstance(text, str): return 0.0
    m = re.findall(r'(\d+(?:\.\d+)?)\s*(?:ml|Ù…Ù„|Ù…Ù„ÙŠ)', text.lower())
    return float(m[0]) if m else 0.0


def extract_brand(text):
    if not isinstance(text, str): return ""
    n = normalize(text)
    tl = text.lower()
    for b in ALL_BRANDS:
        nb = normalize(b)
        if nb and (nb in n or b.lower() in tl):
            return b
    return ""


def extract_type(text):
    if not isinstance(text, str): return ""
    n = normalize(text)
    if "extrait" in n: return "EXTRAIT"
    if "edp" in n: return "EDP"
    if "edt" in n: return "EDT"
    if "edc" in n: return "EDC"
    return ""


def is_sample(t):
    return isinstance(t, str) and any(k in t.lower() for k in REJECT_KEYWORDS)


def best_col(df, cands):
    """Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø¹Ù…ÙˆØ¯ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±Ø´Ø­ÙŠÙ†"""
    for c in cands:
        if c in df.columns: return c
    return df.columns[0] if len(df.columns) else ""


def get_price(row):
    for c in ["Ø§Ù„Ø³Ø¹Ø±", "Ø³Ø¹Ø±", "Price", "price", "PRICE"]:
        if c in row.index:
            try: return float(str(row[c]).replace(",", "").replace(" ", ""))
            except Exception: pass
    return 0.0


def get_id(row, col):
    if not col or col not in row.index: return ""
    v = str(row.get(col, ""))
    return "" if v in ("nan", "None", "") else v


# â•â• ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§ÙØ³ (pre-normalized) â•â•â•â•â•â•â•â•â•â•â•â•â•
class CompIndex:
    """ÙŠÙØ¨Ù†Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© â€” ÙŠØ³Ø±Ù‘Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© 5x"""
    def __init__(self, df, name_col, id_col, comp_name):
        self.comp_name  = comp_name
        self.raw_names  = df[name_col].fillna("").astype(str).tolist()
        self.norm_names = [normalize(n) for n in self.raw_names]
        self.brands     = [extract_brand(n) for n in self.raw_names]
        self.sizes      = [extract_size(n) for n in self.raw_names]
        self.types      = [extract_type(n) for n in self.raw_names]
        self.prices     = [get_price(row) for _, row in df.iterrows()]
        self.ids        = [get_id(row, id_col) for _, row in df.iterrows()]
        self._valid     = [i for i, n in enumerate(self.raw_names) if not is_sample(n) and n.strip()]

    def search(self, our_norm, our_br, our_sz, our_tp, top_n=5):
        if not self._valid: return []
        valid_norms = [self.norm_names[i] for i in self._valid]

        # Ù…Ø±Ø­Ù„Ø© 1: Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¨Ù€ token_set_ratio
        fast = rf_process.extract(our_norm, valid_norms,
                                   scorer=fuzz.token_set_ratio,
                                   limit=min(25, len(valid_norms)))
        cands, seen = [], set()
        for _, fast_sc, vi in fast:
            if fast_sc < max(MATCH_THRESHOLD - 15, 40): continue
            idx  = self._valid[vi]
            name = self.raw_names[idx]
            if name in seen: continue

            c_br = self.brands[idx]
            c_sz = self.sizes[idx]
            c_tp = self.types[idx]

            # â”€â”€ ÙÙ„Ø§ØªØ± ØµØ§Ø±Ù…Ø© â”€â”€
            if our_br and c_br and normalize(our_br) != normalize(c_br): continue
            if our_sz > 0 and c_sz > 0 and abs(our_sz - c_sz) > 30: continue
            if our_tp and c_tp and our_tp != c_tp:
                if our_sz > 0 and c_sz > 0 and abs(our_sz - c_sz) <= 3: continue

            # â”€â”€ score Ù…Ø±ÙƒÙ‘Ø¨ Ø¯Ù‚ÙŠÙ‚ â”€â”€
            n1, n2 = our_norm, self.norm_names[idx]
            s = (fuzz.token_sort_ratio(n1, n2) * 0.30
               + fuzz.token_set_ratio(n1, n2)  * 0.40
               + fuzz.partial_ratio(n1, n2)    * 0.30)

            if our_br and c_br:
                s += 8 if normalize(our_br) == normalize(c_br) else -22
            if our_sz > 0 and c_sz > 0:
                d = abs(our_sz - c_sz)
                s += 8 if d == 0 else (-5 if d <= 5 else -15 if d <= 20 else -28)
            if our_tp and c_tp and our_tp != c_tp:
                s -= 14

            score = round(max(0, min(100, s)), 1)
            if score < MATCH_THRESHOLD: continue
            seen.add(name)
            cands.append({
                "name": name, "score": score,
                "price": self.prices[idx], "product_id": self.ids[idx],
                "brand": c_br, "size": c_sz, "type": c_tp,
                "competitor": self.comp_name,
            })

        cands.sort(key=lambda x: x["score"], reverse=True)
        return cands[:top_n]


# â•â• Gemini Batch â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _ai_batch(batch):
    if not GEMINI_API_KEYS or not batch:
        return [0] * len(batch)

    ck = hashlib.md5(json.dumps(
        [{"o": x["our"], "c": [c["name"] for c in x["candidates"]]} for x in batch],
        ensure_ascii=False, sort_keys=True).encode()).hexdigest()
    cached = _cget(ck)
    if cached is not None:
        return cached

    lines = []
    for i, it in enumerate(batch):
        cands_text = "\n".join(
            f"  {j+1}. {c['name']} | {int(c.get('size',0))}ml | {c.get('type','?')} | {c.get('price',0):.0f}Ø±.Ø³"
            for j, c in enumerate(it["candidates"])
        )
        lines.append(f"[{i+1}] Ù…Ù†ØªØ¬Ù†Ø§: Â«{it['our']}Â» ({it['price']:.0f}Ø±.Ø³)\n{cands_text}")

    prompt = (
        "Ø®Ø¨ÙŠØ± Ø¹Ø·ÙˆØ± ÙØ§Ø®Ø±Ø©. Ù„ÙƒÙ„ Ù…Ù†ØªØ¬ Ø§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø´Ø­ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ Ø£Ùˆ 0 Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯.\n"
        "Ø´Ø±ÙˆØ·: âœ“Ù†ÙØ³ Ø§Ù„Ù…Ø§Ø±ÙƒØ© âœ“Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù… (Â±5ml) âœ“Ù†ÙØ³ EDP/EDT Ø¥Ø°Ø§ Ù…Ø°ÙƒÙˆØ±\n\n"
        + "\n\n".join(lines)
        + f'\n\nJSON ÙÙ‚Ø·: {{"results":[r1,r2,...,r{len(batch)}]}}'
    )
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0, "maxOutputTokens": 300, "topP": 1, "topK": 1}
    }
    for attempt in range(3):
        for key in GEMINI_API_KEYS:
            if not key: continue
            try:
                r = _req.post(f"{_GURL}?key={key}", json=payload, timeout=25)
                if r.status_code == 200:
                    txt = r.json()["candidates"][0]["content"]["parts"][0]["text"]
                    clean = re.sub(r'```json|```', '', txt).strip()
                    s = clean.find('{'); e = clean.rfind('}') + 1
                    if s >= 0 and e > s:
                        raw = json.loads(clean[s:e]).get("results", [])
                        out = []
                        for j, it in enumerate(batch):
                            n = raw[j] if j < len(raw) else 1
                            try: n = int(n)
                            except Exception: n = 1
                            if 1 <= n <= len(it["candidates"]): out.append(n - 1)
                            elif n == 0: out.append(-1)
                            else: out.append(0)
                        _cset(ck, out)
                        return out
                elif r.status_code == 429:
                    time.sleep(2 ** attempt)
            except Exception:
                continue
        time.sleep(1)
    return [0] * len(batch)


# â•â• Ø¨Ù†Ø§Ø¡ ØµÙ Ø§Ù„Ù†ØªÙŠØ¬Ø© â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _build_row(product, our_price, our_id, brand, size, ptype,
               best=None, src="", all_cands=None):
    sz_str = f"{int(size)}ml" if size else ""
    base = dict(Ø§Ù„Ù…Ù†ØªØ¬=product, Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†ØªØ¬=our_id, Ø§Ù„Ø³Ø¹Ø±=our_price,
                Ø§Ù„Ù…Ø§Ø±ÙƒØ©=brand, Ø§Ù„Ø­Ø¬Ù…=sz_str, Ø§Ù„Ù†ÙˆØ¹=ptype)
    if best is None:
        return {**base, **{
            "Ù…Ù†ØªØ¬_Ø§Ù„Ù…Ù†Ø§ÙØ³": "â€”", "Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†Ø§ÙØ³": "", "Ø³Ø¹Ø±_Ø§Ù„Ù…Ù†Ø§ÙØ³": 0.0,
            "Ø§Ù„ÙØ±Ù‚": 0.0, "Ø§Ù„ÙØ±Ù‚_Ø¨Ø§Ù„Ù†Ø³Ø¨Ø©": 0.0, "Ù†Ø³Ø¨Ø©_Ø§Ù„ØªØ·Ø§Ø¨Ù‚": 0.0,
            "Ø§Ù„Ù‚Ø±Ø§Ø±": "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³", "Ø§Ù„Ø®Ø·ÙˆØ±Ø©": "",
            "Ø§Ù„Ù…Ù†Ø§ÙØ³": "", "Ù…ØµØ¯Ø±_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©": "â€”", "Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†": [],
        }}

    cp    = float(best.get("price") or 0)
    score = float(best.get("score") or 0)
    diff  = round(our_price - cp, 2) if our_price > 0 and cp > 0 else 0.0
    pct   = round(diff / cp * 100, 1) if cp > 0 else 0.0
    risk  = "ğŸ”´ Ø¹Ø§Ù„ÙŠ" if abs(diff) > 30 else ("ğŸŸ¡ Ù…ØªÙˆØ³Ø·" if abs(diff) > 10 else "ğŸŸ¢ Ù…Ù†Ø®ÙØ¶")

    if abs(diff) <= PRICE_TOLERANCE:   dec = "âœ… Ù…ÙˆØ§ÙÙ‚ Ø¹Ù„ÙŠÙ‡Ø§"
    elif diff > PRICE_TOLERANCE:       dec = "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰"
    else:                              dec = "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„"

    if score < REVIEW_THRESHOLD and src not in ("auto", "gemini"):
        dec = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"

    src_label = {"auto": f"âš¡ ØªÙ„Ù‚Ø§Ø¦ÙŠ({score:.0f}%)", "gemini": f"ğŸ¤– AI({score:.0f}%)"}.get(src, f"{score:.0f}%")

    return {**base, **{
        "Ù…Ù†ØªØ¬_Ø§Ù„Ù…Ù†Ø§ÙØ³": best["name"],
        "Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†Ø§ÙØ³": best.get("product_id", ""),
        "Ø³Ø¹Ø±_Ø§Ù„Ù…Ù†Ø§ÙØ³":  cp,
        "Ø§Ù„ÙØ±Ù‚":        diff,
        "Ø§Ù„ÙØ±Ù‚_Ø¨Ø§Ù„Ù†Ø³Ø¨Ø©": pct,
        "Ù†Ø³Ø¨Ø©_Ø§Ù„ØªØ·Ø§Ø¨Ù‚": score,
        "Ø§Ù„Ù‚Ø±Ø§Ø±":       dec,
        "Ø§Ù„Ø®Ø·ÙˆØ±Ø©":      risk,
        "Ø§Ù„Ù…Ù†Ø§ÙØ³":      best.get("competitor", ""),
        "Ù…ØµØ¯Ø±_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©": src_label,
        "Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†": (all_cands or [best])[:5],
    }}


# â•â• Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_analysis(our_df, comp_dfs, progress_cb=None, use_ai=True):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ â€” Ø³Ø±ÙŠØ¹ + Ø¯Ù‚ÙŠÙ‚
    our_df: DataFrame Ù…Ù„Ù Ù…Ù‡ÙˆÙˆØ³ (ÙŠØ­ØªÙˆÙŠ Ø£Ø¹Ù…Ø¯Ø©: Ø§Ù„Ù…Ù†ØªØ¬ØŒ Ø§Ù„Ø³Ø¹Ø±ØŒ Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†ØªØ¬)
    comp_dfs: {Ø§Ø³Ù…: DataFrame} Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
    progress_cb: Ø¯Ø§Ù„Ø©(0.0â†’1.0)
    """
    results = []
    our_name_col  = best_col(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Product", "Name", "name", "Ø§Ø³Ù…"])
    our_price_col = best_col(our_df, ["Ø§Ù„Ø³Ø¹Ø±", "Ø³Ø¹Ø±", "Price", "price", "PRICE"])
    our_id_col    = best_col(our_df, ["Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†ØªØ¬", "no", "NO", "No", "Ù…Ø¹Ø±Ù", "ID", "id", "SKU", "sku", "Ø§Ù„ÙƒÙˆØ¯"])

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
    indices = {}
    for cname, cdf in comp_dfs.items():
        cn_col = best_col(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Product", "Name", "name", "Ø§Ø³Ù…"])
        ci_col = best_col(cdf, ["ID", "id", "Ù…Ø¹Ø±Ù", "SKU", "sku", "Ø§Ù„ÙƒÙˆØ¯", "code", "no", "NO"])
        indices[cname] = CompIndex(cdf, cn_col, ci_col, cname)

    total   = len(our_df)
    pending = []

    def flush():
        if not pending: return
        idxs = _ai_batch(pending)
        for j, it in enumerate(pending):
            ci = idxs[j] if j < len(idxs) else 0
            if ci < 0:
                results.append(_build_row(it["product"], it["our_price"], it["our_id"],
                                           it["brand"], it["size"], it["ptype"], src="gemini"))
            else:
                best = it["candidates"][ci]
                results.append(_build_row(it["product"], it["our_price"], it["our_id"],
                                           it["brand"], it["size"], it["ptype"],
                                           best=best, src="gemini", all_cands=it["all_cands"]))
        pending.clear()

    for i, (_, row) in enumerate(our_df.iterrows()):
        product = str(row.get(our_name_col, "")).strip()
        if not product or is_sample(product):
            if progress_cb: progress_cb((i + 1) / total)
            continue

        try: our_price = float(str(row.get(our_price_col, 0)).replace(",", ""))
        except Exception: our_price = 0.0
        our_id = get_id(row, our_id_col)
        brand  = extract_brand(product)
        size   = extract_size(product)
        ptype  = extract_type(product)
        our_n  = normalize(product)

        # Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
        all_cands = []
        for idx_obj in indices.values():
            all_cands.extend(idx_obj.search(our_n, brand, size, ptype, top_n=5))

        if not all_cands:
            results.append(_build_row(product, our_price, our_id, brand, size, ptype))
            if progress_cb: progress_cb((i + 1) / total)
            continue

        all_cands.sort(key=lambda x: x["score"], reverse=True)
        best = all_cands[0]

        if best["score"] >= AUTO_THRESHOLD or not use_ai:
            results.append(_build_row(product, our_price, our_id, brand, size, ptype,
                                       best=best, src="auto", all_cands=all_cands))
        else:
            pending.append(dict(product=product, our_price=our_price, our_id=our_id,
                                brand=brand, size=size, ptype=ptype,
                                candidates=all_cands[:5], all_cands=all_cands,
                                our=product, price=our_price))
            if len(pending) >= AI_BATCH_SIZE:
                flush()

        if progress_cb: progress_cb((i + 1) / total)

    flush()
    return pd.DataFrame(results)


# â•â• Ù…Ù†ØªØ¬Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø© â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def find_missing(our_df, comp_dfs):
    our_name_col = best_col(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Product", "Name", "name"])
    our_norms    = [normalize(str(r.get(our_name_col, "")))
                    for _, r in our_df.iterrows()
                    if not is_sample(str(r.get(our_name_col, "")))]

    missing, seen = [], set()
    for cname, cdf in comp_dfs.items():
        cn_col = best_col(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Product", "Name", "name"])
        ci_col = best_col(cdf, ["ID", "id", "Ù…Ø¹Ø±Ù", "SKU", "sku", "Ø§Ù„ÙƒÙˆØ¯", "code"])
        for _, row in cdf.iterrows():
            cp = str(row.get(cn_col, "")).strip()
            if not cp or is_sample(cp): continue
            cn = normalize(cp)
            if not cn or cn in seen: continue
            m = rf_process.extractOne(cn, our_norms, scorer=fuzz.token_sort_ratio, score_cutoff=70)
            if m: continue
            seen.add(cn)
            sz = extract_size(cp)
            missing.append({
                "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": cp,
                "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³": get_id(row, ci_col),
                "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³":  get_price(row),
                "Ø§Ù„Ù…Ù†Ø§ÙØ³":       cname,
                "Ø§Ù„Ù…Ø§Ø±ÙƒØ©":       extract_brand(cp),
                "Ø§Ù„Ø­Ø¬Ù…":         f"{int(sz)}ml" if sz else "",
                "Ø§Ù„Ù†ÙˆØ¹":         extract_type(cp),
            })
    return pd.DataFrame(missing) if missing else pd.DataFrame()


# â•â• ØªØµØ¯ÙŠØ± Excel Ù…Ù„ÙˆÙ‘Ù† â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_excel(df, sheet="Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
    """ØªØµØ¯ÙŠØ± Ù…Ø¹ Ø£Ù„ÙˆØ§Ù† Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±"""
    from openpyxl.styles import PatternFill, Font, Alignment
    from openpyxl.utils import get_column_letter
    output = io.BytesIO()
    edf = df.copy()
    for c in ["Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†", "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†"]:
        if c in edf.columns: edf.drop(columns=[c], inplace=True)
    sheet_name = str(sheet)[:31]
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        edf.to_excel(writer, sheet_name=sheet_name, index=False)
        ws = writer.sheets[sheet_name]
        hf    = PatternFill("solid", fgColor="1e293b")
        hfont = Font(color="FFFFFF", bold=True, size=10)
        for cell in ws[1]:
            cell.fill = hf; cell.font = hfont
            cell.alignment = Alignment(horizontal="center")
        COLORS = {
            "ğŸ”´": "FFE5E5", "ğŸŸ¢": "E5FFE5",
            "âœ…": "E5FFF5", "âš ï¸": "FFF8E5", "ğŸ”µ": "E5F0FF"
        }
        dcol = None
        for i, cell in enumerate(ws[1], 1):
            if "Ø§Ù„Ù‚Ø±Ø§Ø±" in str(cell.value or ""): dcol = i; break
        if dcol:
            for ri in range(2, ws.max_row + 1):
                val = str(ws.cell(ri, dcol).value or "")
                for emoji, color in COLORS.items():
                    if emoji in val:
                        for ci in range(1, ws.max_column + 1):
                            ws.cell(ri, ci).fill = PatternFill("solid", fgColor=color)
                        break
        for ci, col in enumerate(ws.columns, 1):
            w = max(len(str(c.value or "")) for c in col)
            ws.column_dimensions[get_column_letter(ci)].width = min(w + 4, 55)
    return output.getvalue()
